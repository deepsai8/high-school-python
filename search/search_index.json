{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Applied Data Science","text":"<p>Here you'll learn all about Applied Data Science in a practical manner https://deepsai8.github.io/high-school-python/.</p>"},{"location":"#links","title":"Links","text":""},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file .\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"#plotly-plots","title":"Plotly plots","text":"{     \"data\": [         {             \"x\": [\"giraffes\", \"orangutans\", \"monkeys\"],             \"y\": [20, 14, 23],             \"type\": \"bar\"         }     ] }"},{"location":"#plotly-plot-2","title":"Plotly plot 2","text":"{     \"data\": [         {             \"x\": [\"raccoons\", \"pandas\", \"wolves\"],             \"y\": [2, 20, 5],             \"type\": \"bar\"         }     ] }"},{"location":"#initial-thoughts","title":"Initial Thoughts","text":""},{"location":"#initial-thoughts_1","title":"Initial Thoughts:","text":""},{"location":"#ill-try-to-make-things-as-intuitive-as-possible","title":"I'll try to make things as intuitive as possible!","text":""},{"location":"markmap_01/","title":"Markmap","text":""},{"location":"blogs/","title":"Index","text":""},{"location":"blogs/#blogs-here","title":"Blogs here","text":""},{"location":"dbms/psycopg/","title":"psycopg","text":""},{"location":"dbms/psycopg/#psycopg2","title":"Psycopg2","text":"<p>It is a package that can be used to integrate PostgreSQL with Jupyter Lab/Notebook</p> <pre><code>l1 = [1, 2, 3, 4]\n</code></pre> <pre><code>l1.pop()\nl1\n</code></pre> <pre><code>[1, 2]\n</code></pre> \\n  \\n    \\n      \\n      IndexType\\n      Scan\\n      EqualitySearch\\n      RangeSearch\\n      Insert\\n      Delete\\n    \\n  \\n  \\n    \\n      0\\n      no_index\\n      BD\\n      0.5BD\\n      BD\\n      2D\\n      Search + D\\n    \\n    \\n      1\\n      hash\\n      BD(R + 0.125) + 2BRC\\n      2D\\n      BD\\n      4D\\n      Search + 2D\\n    \\n    \\n      2\\n      btree\\n      1.5BD\\n      D$\\\\log_F$(1.5B)\\n      D$\\\\log_F$(1.5B) + $log_2$(R) * C\\n      Search + D\\n      Search + D\\n    \\n  \\n <pre><code>import pandas as pd\n</code></pre> <pre><code>d = {'IndexType':['no_index', 'hash', 'btree'],\n    'Scan':['BD', 'BD(R + 0.125) + 2BRC', '1.5BD'],\n     'EqualitySearch': ['0.5BD', '2D', r'D$\\log_F$(1.5B)'],\n     'RangeSearch': ['BD', 'BD', r'D$\\log_F$(1.5B) + $log_2$(R) * C'],\n     'Insert': ['2D', '4D', 'Search + D'],\n     'Delete': ['Search + D', 'Search + 2D', 'Search + D']\n    }\n</code></pre> <pre><code>pd.DataFrame(d).to_html()\n</code></pre> <pre><code>'&lt;table border=\"1\" class=\"dataframe\"&gt;\\n  &lt;thead&gt;\\n    &lt;tr style=\"text-align: right;\"&gt;\\n      &lt;th&gt;&lt;/th&gt;\\n      &lt;th&gt;IndexType&lt;/th&gt;\\n      &lt;th&gt;Scan&lt;/th&gt;\\n      &lt;th&gt;EqualitySearch&lt;/th&gt;\\n      &lt;th&gt;RangeSearch&lt;/th&gt;\\n      &lt;th&gt;Insert&lt;/th&gt;\\n      &lt;th&gt;Delete&lt;/th&gt;\\n    &lt;/tr&gt;\\n  &lt;/thead&gt;\\n  &lt;tbody&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;0&lt;/th&gt;\\n      &lt;td&gt;no_index&lt;/td&gt;\\n      &lt;td&gt;BD&lt;/td&gt;\\n      &lt;td&gt;0.5BD&lt;/td&gt;\\n      &lt;td&gt;BD&lt;/td&gt;\\n      &lt;td&gt;2D&lt;/td&gt;\\n      &lt;td&gt;Search + D&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;1&lt;/th&gt;\\n      &lt;td&gt;hash&lt;/td&gt;\\n      &lt;td&gt;BD(R + 0.125) + 2BRC&lt;/td&gt;\\n      &lt;td&gt;2D&lt;/td&gt;\\n      &lt;td&gt;BD&lt;/td&gt;\\n      &lt;td&gt;4D&lt;/td&gt;\\n      &lt;td&gt;Search + 2D&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;2&lt;/th&gt;\\n      &lt;td&gt;btree&lt;/td&gt;\\n      &lt;td&gt;1.5BD&lt;/td&gt;\\n      &lt;td&gt;D$\\\\log_F$(1.5B)&lt;/td&gt;\\n      &lt;td&gt;D$\\\\log_F$(1.5B) + $log_2$(R) * C&lt;/td&gt;\\n      &lt;td&gt;Search + D&lt;/td&gt;\\n      &lt;td&gt;Search + D&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;'\n</code></pre> <pre><code>d = {'eid':[1, 2, 3],'name':['Jackson', 'Jonathan', 'Nabi'], 'title':['Director', 'Professor', 'Engineer'], 'ssn':[None, None, None]}\ndf = pd.DataFrame(d)\ndf.to_csv('employees_2.csv', index = False)\n</code></pre> <pre><code>! pwd\n</code></pre> <pre><code>/Users/deepaksingh/Desktop/MSDS/RDBMS\n</code></pre> <pre><code>! conda env list\n</code></pre> <pre><code># conda environments:\n#\nbase                  *  /Users/deepaksingh/opt/anaconda3\ndiscom                   /Users/deepaksingh/opt/anaconda3/envs/discom\nenv-cassandra            /Users/deepaksingh/opt/anaconda3/envs/env-cassandra\nenv-obb                  /Users/deepaksingh/opt/anaconda3/envs/env-obb\nenv-tensors              /Users/deepaksingh/opt/anaconda3/envs/env-tensors\n</code></pre> <pre><code>df = pd.read_csv('Iowa_Fleet_Summary_By_Year__County_And_Vehicle_Type.csv')\ndf.head()\n</code></pre> Year Year Ending County Name County FIP Feature ID Motor Vehicle Vehicle Category Vehicle Type Tonnage Registrations Annual Fee Primary County Lat Primary County Long Primary County Coordinates 0 2005 12/31/2005 No County NaN NaN Yes Automobile Automobile NaN 3711 NaN NaN NaN NaN 1 2006 12/31/2006 No County NaN NaN Yes Motor Home Motor Home - A NaN 1 NaN NaN NaN NaN 2 2008 12/31/2008 No County NaN NaN Yes Automobile Automobile NaN 14 1021.0 NaN NaN NaN 3 2008 12/31/2008 Ida 19093.0 465235.0 Yes Bus Bus NaN 5 680.0 42.386875 -95.513496 POINT (-95.5134962 42.3868747) 4 2011 12/31/2011 Jasper 19099.0 465238.0 Yes Moped Moped NaN 198 1386.0 41.686039 -93.053765 POINT (-93.053765 41.6860394) <pre><code>import psycopg2 as pg\n</code></pre> <pre><code>host = 'localhost'\nport = 5432\ndatabase = 'msds691'\nuser = 'postgres'\npassword = 'postgres'\n</code></pre> <pre><code>conn = pg.connect(host = host, port = port, database = database, user = user, password = password)\n</code></pre> <pre><code>cur = conn.cursor()\n</code></pre> <pre><code>cur.execute(\"select * from employees2\")\ncur.fetchall()\n</code></pre> <pre><code>[(1, 'Jackson', 'Director', None),\n (2, 'Jonathan', 'Professor', None),\n (3, 'Nabi', 'Engineer', None)]\n</code></pre> <pre><code>cur.execute(\"select department_id from departments;\")\ndata = cur.fetchall()\ndid_list = []\nfor i in data:\n    did_list.append(i[0])\ndid_list\n</code></pre> <pre><code>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>conn\n</code></pre> <pre><code>&lt;connection object at 0x137a48c10; dsn: 'user=postgres password=xxx dbname=msds691 host=localhost port=5432', closed: 0&gt;\n</code></pre> <pre><code>cur.close()\nconn.close()\n</code></pre> <pre><code>\n</code></pre>"},{"location":"dbms/sql_theory/","title":"SQL Theory","text":""},{"location":"dbms/sql_theory/#environment-setup","title":"Environment Setup","text":""},{"location":"dbms/sql_theory/#conda-environment-commands","title":"Conda environment commands","text":"<p>conda create --name DistributedComputing python=3 -y</p> <p>conda activate DistributedComputing</p> <p>conda info --envs (this will list all available envs) </p> <p>conda list -n DistributedComputing (this will list all packages in the env)</p> <p>conda deactivate</p> <p>conda env list (this will list all available envs)</p> <p>conda env remove --name DistributedComputing</p> <p>conda env export  -n DistributedComputing --no-build &gt; DistributedComputing_environment.yml</p> <p>conda env create -f DistributedComputing_environment.yml -n DistributedComputing </p> <p>conda env update -f DistributedComputing_environment.yml -n DistributedComputing</p>"},{"location":"dbms/sql_theory/#test-suit","title":"test suit","text":"<ul> <li>pytest -vv</li> <li>pycodestyle hw1.py</li> <li>autopep8 --in-place --aggressive &gt;</li> </ul>"},{"location":"dbms/sql_theory/#git","title":"Git","text":"<ul> <li>git remote -v (to check the url to push and pull data)</li> </ul>"},{"location":"dbms/sql_theory/#installation-of-postgresql","title":"Installation of postgresql","text":"<p>on terminal: - setup <pre><code>$ brew install postgresql@15\n</code></pre> - initialize db (create the dirs if required) <pre><code>$ initdb /usr/local/var/postgres \n</code></pre> - start the server on your local machine <pre><code>$ postgres -D /usr/local/var/postgres\n</code></pre> - to use terminal as an interface <pre><code>$ psql postgres \n</code></pre> </p> <p>Connections using host, port, username(postgres) - I would recommend installing one of these     - pgadmin     - DBeaver     - SQuirrel     - to use postgresql on jupyter lab/notebooks: <code>$ conda install psycopg2</code></p>"},{"location":"dbms/sql_theory/#database","title":"Database","text":"<p>A database is an organized collection of structured information, or data, typically stored electronically in a computer system 1</p>"},{"location":"dbms/sql_theory/#dbms","title":"DBMS","text":"<p>A DBMS serves as an interface between the database and its end users or programs, allowing users to retrieve, update, and manage how the information is organized and optimized. A DBMS also facilitates oversight and control of databases, enabling a variety of administrative operations such as performance monitoring, tuning, and backup and recovery.1</p>"},{"location":"dbms/sql_theory/#why-dbms","title":"Why DBMS","text":"<ol> <li>Data Independence : The DBMS provides an abstract view of the data that hides details of data representation and storage.</li> <li>Efficient Data Access : Store and retrieve data efficiently.</li> <li>Data Integrity and Security : Enforce data integrity and access controls that governs what data is visible to different classes of users.</li> <li>Data Administration : When several users share the data, it centralize the administration of data.</li> <li>Concurrent Access and Crash Recovery : A DBMS schedules concurrent access to the data in such a manner that users can think of the data accessed by one user at a time. And The DBMS protects from the effects of system failures.</li> <li>Reduced Application and Development Time : Provides functions to access data.</li> </ol>"},{"location":"dbms/sql_theory/#rdbms","title":"RDBMS","text":"<ul> <li>A relational database is a (most commonly digital) database based on the relational model of data, as proposed by E. F. Codd in 1970 23</li> <li>Based on a relational data model</li> </ul>"},{"location":"dbms/sql_theory/#relational-model","title":"Relational Model","text":"<ul> <li>A relational model organizes data into one or more tables (or \"relations\") of columns and rows, with a unique key identifying each row. Rows are also called records or tuples. Columns are also called attributes. Generally, each table/relation represents one \"entity type\" (such as customer or product). The rows represent instances of that type of entity (such as \"Lee\" or \"chair\") and the columns represent values attributed to that instance (such as address or price)</li> <li>Every relation has a schema or a description of the data which specifies its name, column/attribute/field name, type of data (e.g. string, int etc.), and integrity constraints (e.g. primary keys, unique keys foreign keys, other conditions etc.)</li> </ul> RDBMS ER model"},{"location":"dbms/sql_theory/#6-steps-of-relational-db-design","title":"6 Steps of Relational DB Design","text":"<ol> <li>Requirements Analysis: find out what the users want from the DB</li> <li>Conceptual DB Design: Develop a high-level description of the data to be stored (aka ER or Entity-Relationship diagram)</li> <li>Logical DB Design: Convert the conceptual DB design into a DB schema</li> <li>Schema Refinement: Identify potential problems and refine it</li> <li>Physical DB Design: Ensure that the DB design meets desired performance criteria</li> <li>Security Design: Define roles and identify the parts of the DB that should/should not be accessible by those roles</li> </ol>"},{"location":"dbms/sql_theory/#er-model-entity-relationship","title":"ER Model (Entity-Relationship)","text":"<ul> <li>This is done to describe entities or objects, their relationships and the constraints</li> <li>Entities are objects distinguishable from other objects</li> <li>An entity os described using a set of attributes</li> <li>Each attribute in an entity has a domain of possible values</li> <li>Primary key can be defined from here as:<ul> <li>Primary Key : A minimal set of attributes whose values uniquely identify an entity in the set</li> </ul> </li> <li>Relationships are association among two or more entities</li> <li>Constraints<ul> <li>Key constraints : Restrict 1-1, 1-many, and many-many relationships</li> <li>Participation Constraints : partial and total constraint<ul> <li>Total - Every instance of an entity is participating the relationship</li> <li>Partial - The participation of an entity in the relationship is not total</li> </ul> </li> </ul> </li> </ul> <p>Notes: - ER designs are subjective in nature - meaning, there are multiple possible choices of design</p>"},{"location":"dbms/sql_theory/#sql","title":"SQL","text":"<p>Most widely used language for creating, manipulating and querying RDBMS</p> <ul> <li>RDBMS uses dot notation when referring to database objects;<ul> <li>e.g. database.schema.table.column</li> </ul> </li> <li>SQL ignores hard returns, additional white spaces and is generally case insensitive</li> <li>A statement ends with semicolon ;</li> </ul>"},{"location":"dbms/sql_theory/#ddl","title":"DDL","text":"<ul> <li>Create a DB: <code>CREATE DATABASE database_name;</code></li> <li>Create a schema: <code>CREATE SCHEMA schema_name;</code></li> </ul>"},{"location":"dbms/sql_theory/#crud","title":"CRUD","text":"<ul> <li>Create</li> <li>Read</li> <li>Update</li> <li>Delete</li> </ul>"},{"location":"dbms/sql_theory/#keys","title":"Keys","text":"<p>Super key: Attributes that can uniquely identify a tuple</p> <p>Candidate Key: Minimal subset of Attributes that can uniquely identify a tuple</p> <p>Unique Key:  - Minimal subset of Columns that can uniquely identify a tuple - Can be null - Table can have multiple unique keys</p> <p>Primary Key - Minimal subset of Columns that can uniquely identify a tuple - Can Not be null - Only one primary key exists in a table</p> <p>Foreign Key - Info stored in a relation linked to the info stored in another relation - If one gets modified, the other must be modified/checked to see consistency - Foreign key in the referencing relation must match primay key in the referenced relation, however, names can be different</p> <p>The data definition language deals with the schema creation and modification e.g., CREATE TABLE statement allows you to create a new table in the database and the ALTER TABLE statement changes the structure of an existing table.</p> <p>The data manipulation language provides the constructs to query data such as the SELECT statement and to update the data such as INSERT, UPDATE, and DELETE statements.</p> <p>The data control language consists of the statements that deal with the user authorization and security such as GRANT and REVOKE statements.</p>"},{"location":"dbms/sql_theory/#data-types","title":"Data Types","text":"RDB Design  RDB Design   #### Cast operator - cast(expression as target_type) - expression::target_type  #### Type Conversion    RDB Design   ### Functions    RDB Design  RDB Design  RDB Design      ## Index Algorithms  ### Database performance Best way to store/find data for better performance  -DBMS: collection of records or a file, and each file consisting of one or more pages - Page: Unit of information read from or written to disk     - Page I/O dominates the cost of typical database operations     - If we read pages in the order that the data is stored physically, the cost can be much less - Record: Each record in a file has a unique identifier (Record ID or rid), to identify the address of the page containing the record     - using rid, DBMS fetches the page including data from disk to memory  Best way to organize records in a file when the file is stored on the disk for fast access to desired subset of records  ### Index: - Data structure that organized data records on disk to optimize data retrievals using search key - A good indexing algorithm improves query performance significantly  Some terminology: - a data entry, k* is a data records stored in an index file     - a data entry has a search key value k     - a data entry contains information to locate data records with k  Clustered Indexes - order of the data records is the same or close to the order of data entries Unclustered Indexes - order of the data records is not the same or close to the order of data entries  ### Hash Index - the records are grouped in buckets, where buckets consists of pages(s) - the bucket that a record belongs to can be determined by applying hash function to the search key   ### BTree Indexing - Organize records using a self balancing tree structure - data entries are arranged in sorted order by search key value - a hierarchical search data structure is maintained that directs searches to the correct page of data entries - all searches begin at the topmost node, and the contents of pages in non-leaf levels direct search to the correct leaf page - all leaves that contain the data entries are at the same level and are pointed in a doubly-linked list  By default, Postgres doesn't maintain clustered index. We can cluster a table according to an index <pre><code>CLUSTER table_name USING index_name\n</code></pre> - When a table is clustered, it is physically reordered based on the index information.  - Clustering is a one time operation, when the table is subsequently updated, the changes are not clustered    What indexes should we create/use? - consider the most important/frequent query types. consider the best plan using the current indexes, and see if a better plan is possible with an additinal index - attributes in WHERE clause are candidates for index keys     - exact match or range - composit index and their order     - multi-attribute search keys should be considered when a WHERE clause contains several conditions - before creating an index we must also consider the impact on indexes  ### Factors affecting performance - B = # of pages - R = # of records per page - D = Avg time to read or write to a disk page - C = Avg time to process a record - H = Time required to apply the hash function to a record - F = Fan-Out (Avg # of children to a non-leaf node)  Test Results No Index model - Scan = B * (D + R*C) - Search with equality selection = 0.5 * B * (D + R*C) - Search with range selection = B * (D + R*C) - Insert a Record = D + C + D = 2*D + C - Delete a Record = Search with equality + C + D  Hash Index model Assumption: each data entry is 10% of the size of a data record, and pages are about 80% occupancy  Therefore: # of pages required to store data entries:  4/5 full with 10% data entry size =&gt; (5/4 * B) * 0.1 = 0.125 * B  And # of data entries that can fit in a page:  4/5 * R * (1/10%) = 4/5 * R * 10 = 8*R  - Scan = 0.125 * B * (D + 8*R*C) + B*R * (D + C) = BD (R + 0.125) + 2BRC - Search with equality selection = H + D + 0.5 * 8*R*C + D = H + 2*D + 4*R*C - Search with range selection = B * (D + R*C) - Insert a Record = H + 2*D + C + 2*D + C = H + 4*D + 2*C - Delete a Record = Serch with equality + 2*D  BTree Index model (default for Postgress and other DBs) Assumptions: Pages are usually 2/3 occupancy  Therefore: # of pages = 3/2 pages = 1.5 * B  Also, locate the page in $log_F$(1.5*B) * D and search the qualifying record in a page in $log_2$(R) * C - Scan = 1.5 * B * (D + R*C) - Search with equality selection = $log_F$(1.5*B) * D + $log_2$(R) * C - Search with range selection = Equality selection + B - Insert a Record = Search with equality + D - Delete a Record = search with equality + D  d = {'IndexType':['no_index', 'hash', 'btree'],     'Scan':['BD', 'BD(R + 0.125) + 2BRC', '1.5BD'],      'EqualitySearch': ['0.5BD', '2D', r'D$\\log_F$(1.5B)'],      'RangeSearch': ['BD', 'BD', r'D$\\log_F$(1.5B) + $log_2$(R) * C'],      'Insert': ['2D', '4D', 'Search + D'],      'Delete': ['Search + D', 'Search + 2D', 'Search + D']     }  ### Views - a named query that provides a way to present data in the base tables     - the view is not physically materialized, working with the base table     - name of the view must be distinct from the name of any other view, table, index etc in the same schema     - you can query a view just as you do with tables    ### Transaction - A unit of work which bundles multiple steps into a single operation - a transaction is treated in a coherent and reliable way independent of other transactions - foundation for concurrent execution and recovery from system failures  lookup `database transaction` in wikipedia.org and postgresql.org  postgresql: - auto-commit: each successful query is committed after execution - auto-rollback: failed queries are rolled back  ## ACID  ### Atomicity - Actions in transactions are carried out all or none     - DBMS logs all actions so that it can undo the actions of aborted transactions  ### Consistency - A transaction must change affected data, only in allowed ways, according to all defined rules  ### Isolation - When there are several transactions happening concurrently, users should be able to understand a transaction without considering the effect of other concurrently existing transactions  ### Durability - Once a transaction has been successfully completed, its effect should persist even if the system crashes before all its changes are reflected on ddisk  ### Concurrency and Isolation  ### Interleaved Transcations - it allows multiple users of the DB to access it at the same time - When there are multiple transactions, actions(reading, writing, aborting or committing) in transactions could be interleaved to improve performance - Transaction schedule describes a list of actions from a set of transactions as seen by the DBMS     - Serial Schedule: If the actions of different transactions are not interleaved and are executed from start to end one by one     - Serializable Schedule: Over a set of committed transaction is a schedule whose effect is guaranteed to be identical to serial schedule    Clustering   ### Anomalies due to concurrency - Interleaved execution can cause data object conflicts  4 types of conflicts - dirty reads - unrepeatable read - phantom read (special case of unrepeatable read) - lost update  ### Lock - An object that ensures the net effect is identical to executing all transactions in a serial order to minimize anomalies - Types     - shared lock         - Does not prevent other transactions from acquiring the same shared lock         - No update, delete or put an exclusive lock items on which any other transaction holds a shared lock     - exclusive lock         - prevents other transactions from accessing the data  Lock - Performance of locked-based concurrency control     - To resolve conflicts between transactions, it uses blocking and aborting.         - Blocking : holds locks that force other transactions to wait. (Most)         - Aborting : Stops and restarts a transaction. - As more transactions execute concurrently, the likelihood of blocking goes up.     - Solution :          - 1) lock the smallest side objects,         - 2) reducing the time that transaction hold locks,         - 3) reduce hotspots  ## Anomalies due to concurrency  1.Write-Read(WR) Conflict - Dirty Read: Transaction T2 could read a database object A that has been modified by another transaction T1 which has not yet committed. This can lead an inconsistent final database state. - Postgres doesn\u2019t support dirty read.  2.Read-Write(RW) Conflict - Unrepeatable Read : A transaction T2 could change the value of an object that has been read by a transaction T1, while T1 is still in progress. If T1 tries to read the value again, it will get a different result  3.Read-Delete/Insert Conflict - Phantom Reads - Special case of unrepeatable reads. - If the collection of database objects is not fixed, but can grow and shrink through the insertion and deletion of objects, the results differs from serial execution of T1 and T2  4.Write-Write(WW) Conflict - Lost Update : For concurrent transactions, a transaction T2 could overwrite the value of an object A which has already been modified by a transaction T1  ### Durability - Write-Ahead Logging (WAL)     - Any change to a database object is first recorded in the log before the change to the database object is written to disk.     - Postgres way of logging actions.     - A recovery algorithm that ensures data integrity.     - Significantly reduce the number of disk writes, because only the log file needs to be flushed to disk.     - Helps \u201cDurability\u201d of a database.  ## Schema Refinement - Data Redundancy and Anomalies - Decomposition     - 1NF     - 2NF     - 3NF  Recap ### Schema - Schemas contain named objects including data types, functions, and operators. - Help organize database objects into logical groups to make them more manageable. - Are analogous to directories. - You can check schema in your database.     - SELECT *     - FROM information_schema.columns  ### Schema Refinement Data redundancy could cause the following problems. - Redundant storage : Some information is stored repeatedly. - Update anomalies : If one copy of repeated data is updated, an inconsistency is created unless all copy are updated. - Insert anomalies : It may not be possible to store certain information unless some other, unrelated, information is stored as well. - Delete anomalies : It may not be possible to delete certain information without losing some other, unrelated. Information as well.  #### We need to consider Functional Dependency : If column A of a table uniquely identifies the column B of same table, Attribute B is functionally dependent on attribute A (A-&gt;B). - eg hourly_wages are dependent on rating    Clustering   Schema Refinement Data redundancy could cause the following problems. - Redundant storage : The rating value 9 corresponds to the hourly wage 25, and this association is repeated twice. - Update anomalies :The hourly_wage in the first tuple could be updated without making a same change in the second tuple. - Insert anomalies : We cannot insert a tuple unless we know the hourly wage or the employee\u2019s rating. - Delete anomalies : If we delete all tuples with a given rating value, we lose the association between the rating and hours_wage.  ## Decompositions Replacing a relation, R with two or more collection of smaller relations that each contain a subset of the attributes of R and together include all attributes in R. - Functional dependency (FD) and other integrity constraints (ICs) can be used to refine the schema.     - The set of all FDs implied by a given set F of FDs is called the closure of F, denoted as F+.     - Armstrong\u2019s Axiom (3) can be applied repeatedly to infer all FDs implied by a set F of FDs. When X,Y, and Z are sets of attributes over a relation schema R:         - Reflectivity : If Y $\\subseteq$ X, then X =&gt; Y.         - Augmentation : If X =&gt; Y, then XZ =&gt; YZ for any Z.         - Transitivity : If X =&gt; Y and Y =&gt; Z, then X =&gt; Z. - Should satisfy     - Lossless-join: We can recover the original relation from the decomposed relations.     - Dependency-preservation: If R is decomposed into X, Y and Z, and we enforce the FDs that hold on X, on Y and on Z, then all FDs that were given to hold on R must also hold.   On week 1, we designed database tables using an ER diagram. - Defined entity, relationship and their attributes. - Schemas from ER model can offer from redundancy. - Need to refine the initial design by     - 1) Taking the integrity constraints(ICs) into account more fully.     - 2) Considering performance criteria.     - 3) Considering typical workload and query types.  ## Relational Database Design  6 Steps 1. Requirement Analysis : Find out what the users want from the database. 2. Conceptual Database Design : Develop a high-level description of the data to be stored. (ER diagram) 3. Logical Database Design : Convert the conceptual database design into a database schema. 4. Schema Refinement : Identify potential problems and refine it. (Normalization) 5. Physical Database Design : Ensure that the database design meets desired performance criteria. 6. Security Design : Define a role and identify the parts of the database that must/must not be accessible by the role  ## Normalization A technique for organizing data in a database to - Minimize redundancy to reduce redundant storage issues and insert/ update/ delete anomalies. - Ensure data integrity and consistency. - Makes easier to access and maintain the data.  Can be accomplished through Normal Forms (NF).     Clustering   ## Normal Forms ### First Normal Forms (1NF) - A table has a primary key. - A table has no repeating attributes or groups attributes. - Problems      - still have insert/delete/update anomalies.  ### Candidate Keys Recap Week 1. - Super Key - A set of attributes which can uniquely identify a row. - Candidate Key - A minimal set of attributes which can uniquely identify a row. - Primary Key - A minimal set of attributes which can uniquely identify a row. (Cannot be NULL) - Unique Key - A candidate key which is not primary key, and could be multiple unique keys. Could be NULL  ### Second Normal Forms (2NF) - It is a 1NF. - It does not have any non-prime attribute that is functionally dependent on any subset of candidate keys. (Every non-prime attribute is dependent on the whole of every candidate key) - Non-prime attribute : attribute that is not a part of any candidate key - Problems     - slkjdhf  ### Third Normal Forms (3NF) - It is a 2NF. - Every non-prime attribute is non-transitively dependent on every key. - Transitivity : If X Y and Y Z, then X Z - Algorithm     1. Put the FDs in a standard form with a single attribute on the right side.     2. Minimize the left side of each FD while preserving equivalence to F+.     3. Delete redundant FDs.        <ol> <li> <p>Oracle  \u21a9\u21a9</p> </li> <li> <p>Wikipedia RDBMS  \u21a9</p> </li> <li> <p>EF Code 1970 paper  \u21a9</p> </li> </ol>"},{"location":"deeplearning/nn/","title":"Neural Networks","text":"","boost":2},{"location":"deeplearning/nn/#why","title":"Why","text":"<p>Hand engineered features are time consuming, brittle and are not scalable. Can we learn underlying features directly from the data that we have?</p>","boost":2},{"location":"deeplearning/nn/#why-now","title":"Why Now?","text":"","boost":2},{"location":"deeplearning/nn/#we-now-have-all-these","title":"We now have all these:","text":"<ul> <li>Big Data</li> <li>GPU</li> <li>Software (pytorch, keras, tf)</li> <li>Progression:<ul> <li>1952: SGD</li> <li>1995: CNN</li> </ul> </li> </ul>","boost":2},{"location":"probstats/probability/","title":"Probability Theory","text":""},{"location":"probstats/probability/#a-little-bit-of-history","title":"A little bit of History","text":"<p>The modern mathematical theory of probability has its roots in attempts to analyze games of chance by Gerolamo Cardano in the sixteenth century, and by Pierre de Fermat and Blaise Pascal in the seventeenth century (for example the \"problem of points\"). Christiaan Huygens published a book on the subject in 1657. In the 19th century, what is considered the classical definition of probability was completed by Pierre Laplace 1</p>"},{"location":"probstats/probability/#probability","title":"Probability","text":"<p>In science, the probability of an event is a number that indicates how likely the event is to occur. It is expressed as a number in the range from 0 and 1, or, using percentage notation, in the range from 0% to 100%. The more likely it is that the event will occur, the higher its probability.</p>"},{"location":"probstats/probability/#aximoatic-theory","title":"Aximoatic Theory","text":""},{"location":"probstats/probability/#definitions","title":"Definitions","text":""},{"location":"probstats/probability/#deterministic-system","title":"Deterministic System","text":"<p>In mathematics, computer science and physics, a deterministic system is a system in which no randomness is involved in the development of future states of the system. A deterministic model will thus always produce the same output from a given starting condition or initial state.2</p>"},{"location":"probstats/probability/#random-experiment","title":"Random Experiment","text":"<ul> <li>It's a process or an outcome that is not deterministic, i.e. it is stoichastic</li> <li>e.g. \\(f(x) = x^{2}\\) <ul> <li>or fixed input =&gt; fixed output, meaning deterministic</li> </ul> </li> <li>e.g. <ul> <li>Roll a die (6 possibilities)</li> <li>dealing a deck of cards (52! permutations)</li> </ul> </li> </ul>"},{"location":"probstats/probability/#sample-space","title":"Sample Space","text":"<ul> <li>A set of all possible outcomes of a random experiment, typically denoted by S or \\(\\Omega\\)</li> </ul>"},{"location":"probstats/probability/#elements-or-outcomes-or-atoms-or-singletons","title":"Elements or Outcomes (or Atoms or Singletons)","text":"<ul> <li>These are the outcomes. Usually denoted by small 's' or \\(\\omega\\)</li> <li>e.g. Flip a coin 3 times (order matters), will have these outcomes or elements:<ul> <li>HHH, HHT, HTH, HHT, THH, HTT, THT, TTH, TTT</li> </ul> </li> <li>and set1 = {HHH, HHT, HTH} or set2 = {THH, TTH, TTT} or set3 = {TTT} are the events</li> <li>Above entire set of possible outcomes is called a Descrete Random Experiment</li> </ul>"},{"location":"probstats/probability/#event","title":"Event","text":"<ul> <li>Typically denoted by capital letters A, B etc.</li> <li>It is sort of a subset of S, that can be passed on to a probability measure</li> <li>All set operations can be used to build events</li> </ul>"},{"location":"probstats/probability/#probability-measure","title":"Probability Measure","text":"<ul> <li>A notation such as P(A) = P(X&gt;5) is called a probability measure if it satisfies the 3 Axioms of Kolmogorov</li> </ul>"},{"location":"probstats/probability/#kolmogorov-axioms","title":"Kolmogorov Axioms","text":"<p>The Kolmogorov axioms are the foundations of probability theory introduced by Russian mathematician Andrey Kolmogorov in 1933 3. These axioms remain central and have direct contributions to mathematics, the physical sciences, and real-world probability cases. An alternative approach to formalising probability, favoured by some Bayesians, is given by Cox's theorem</p> <ul> <li>Framework: <ul> <li>let S be a sample space for some Random Experiment</li> <li>let P be a function from P : P(S) -&gt; R</li> <li>here, P(S) = Power set = {set of all subsets of S}</li> <li>we say that P is a probabilty measure ans (S, P) is a probabilty space, if the following 3 axioms are satisfied:</li> </ul> </li> </ul>"},{"location":"probstats/probability/#axioms","title":"Axioms","text":"<ul> <li>\\(P(A)\\geq0\\), for any event \\(A \\subseteq S\\)<ul> <li>aka Non-Negativity</li> </ul> </li> <li>P(S) = 1<ul> <li>aka Unitarity or Unity</li> </ul> </li> <li>If \\(A_{1}, A_{2}, ...\\) is a sequence of Mutually Exclusive events, then \\(P\\left(\\bigcup _{i=1}^{\\infty }A_{i}\\right)=\\sum _{i=1}^{\\infty }P(A_{i})\\)<ul> <li>aka \\(\\sigma\\)-Additivity</li> </ul> </li> </ul> <ol> <li> <p>Probabilistic Expectation \u21a9</p> </li> <li> <p>Deterministic System wikipedia \u21a9</p> </li> <li> <p>Foundations of the theory of probability \u21a9</p> </li> <li> <p>By Tim Stellmach - Own work using Inkscape and Open Office Draw software., Public Domain, https://commons.wikimedia.org/w/index.php?curid=1220091 \u21a9</p> </li> </ol>"}]}